{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbd290ee-1d85-432c-8081-3161ea689e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0e93c7a-17c5-4fd1-9d1f-2c56f5004812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_images = idx2numpy.convert_from_file('datasets/train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "train_labels = idx2numpy.convert_from_file('datasets/train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images = idx2numpy.convert_from_file('datasets/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels = idx2numpy.convert_from_file('datasets/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_data = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data = torch.tensor(test_images, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9cfa928a-7945-4794-9191-d867f59036d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 28, 28])\n",
      "Train labels shape: torch.Size([60000])\n",
      "Test data shape: torch.Size([10000, 28, 28])\n",
      "Test labels shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print(\"Train data shape:\", train_data.shape)  # should be (60000, 28, 28)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # should be (60000,)\n",
    "print(\"Test data shape:\", test_data.shape)  # should be (10000, 28, 28)\n",
    "print(\"Test labels shape:\", test_labels.shape)  # should be (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faab6385-9663-485c-875a-52825cca9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pytorch tensors to pytorch datasets\n",
    "train_data = TensorDataset(train_data, train_labels)\n",
    "test_data = TensorDataset(test_data, test_labels)\n",
    "\n",
    "#now translate into dataloader objects\n",
    "batchsize = 128\n",
    "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "904ff70c-7fdb-4f1a-8783-e00494f747c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model function\n",
    "def mnistModel(o='SGD', lr=0.01):\n",
    "\n",
    "    class mnistNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            #input layer\n",
    "            self.input = nn.Linear(784, 64)\n",
    "            \n",
    "            #hidden layer\n",
    "            self.fc1 = nn.Linear(64, 32)\n",
    "            self.fc2 = nn.Linear(32, 32)\n",
    "\n",
    "            #output layer\n",
    "            self.output = nn.Linear(32, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            return self.output(x)\n",
    "\n",
    "    #create model instance\n",
    "    net = mnistNet()\n",
    "\n",
    "    #loss function\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "    #optimizer\n",
    "    optifun = getattr(torch.optim, o)\n",
    "    optimizer = optifun(net.parameters(), lr=lr)\n",
    "\n",
    "    return net, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9eabdfad-b235-48d7-b55c-2fe088b36ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training function\n",
    "def trainModel(o, lr):\n",
    "    epochs = 100\n",
    "\n",
    "    #create a new model\n",
    "    net, lossfun, optimizer = mnistModel(o, lr) \n",
    "\n",
    "    #initialize losses\n",
    "    losses = torch.zeros(epochs)\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    #looping over epochs\n",
    "    for epochi in range(epochs):\n",
    "        batchAcc = []\n",
    "        batchLoss = []\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            #forward pass and loss\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "            \n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss from this batch\n",
    "            batchLoss.append(loss.item())\n",
    "\n",
    "            # compute accuracy\n",
    "            matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "            matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "            accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
    "            batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "            # end of batch loop..\n",
    "\n",
    "        #get their average training accuracy\n",
    "        trainAcc.append( np.mean(batchAcc) )\n",
    "        \n",
    "        # and get average losses across the batches\n",
    "        losses[epochi] = np.mean(batchLoss)\n",
    "        \n",
    "        # test accuracy\n",
    "        X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "        with torch.no_grad(): # deactivates autograd\n",
    "            yHat = net(X)\n",
    "\n",
    "        # compare the following really long line of code to the training accuracy lines\n",
    "        testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "    # function output\n",
    "    return trainAcc,testAcc,losses,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1467e9ce-e382-4c36-b892-452fd8374b61",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_o, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimTypes):\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m idx_lr, lr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(learningRates):\n\u001b[1;32m---> 10\u001b[0m     trainAcc,testAcc,losses,net \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     finalPerformance[idx_lr,idx_o] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(testAcc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[76], line 20\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(o, lr)\u001b[0m\n\u001b[0;32m     16\u001b[0m batchLoss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#forward pass and loss\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     yHat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lossfun(yHat, y)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 19\u001b[0m, in \u001b[0;36mmnistModel.<locals>.mnistNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[1;32mC:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x64)"
     ]
    }
   ],
   "source": [
    "learningRates = [.0001] #add learning rate\n",
    "optimTypes = ['Adam'] #add optimizers here\n",
    "\n",
    "#initialize performance matrix\n",
    "finalPerformance = np.zeros((len(learningRates),len(optimTypes)))\n",
    "\n",
    "# now for the experiment!\n",
    "for idx_o, o in enumerate(optimTypes):\n",
    "  for idx_lr, lr in enumerate(learningRates):\n",
    "    trainAcc,testAcc,losses,net = trainModel(o,lr)\n",
    "    finalPerformance[idx_lr,idx_o] = np.mean(testAcc[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924151b-cc53-4946-9a41-b2d4c70a6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results!\n",
    "plt.plot(learningRates,finalPerformance,'o-',linewidth=2)\n",
    "plt.legend(optimTypes)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning rates')\n",
    "plt.ylabel('Test accuracy (ave. last 10 epochs)')\n",
    "plt.title('Comparison of optimizers by learning rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
